{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Age Classifier with Deep Learning\n",
        "\n",
        "**Описание проекта:**\n",
        "Целью этого проекта была разработка и реализация модели для классификации возраста на основе изображений. Данные были разделены на наборы для обучения и тестирования, где каждый файл изображения представлял одну из четырёх возрастных категорий (0, 1, 2, 3). Основная цель заключалась в создании точной модели, которая способна предсказывать возрастные категории на основе входных изображений, а также оптимизировать её для эффективного использования."
      ],
      "metadata": {
        "id": "9jAZ6pGlolKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1-шаг**: распаковка файла"
      ],
      "metadata": {
        "id": "VCOWzpg6oyjF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLMXdf51y3uQ"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "path = '/content/data.zip'\n",
        "expath = '/content/'\n",
        "\n",
        "with zipfile.ZipFile(path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(expath)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2-шаг**: preprocessing.\n",
        "\n",
        "0. У нас имеются изображения с \"битыми\" именами, их пропускаем (можно просто переименовать), дабы получить нужное нам количество классов. Их 4.\n",
        "1. Сортируем картинки по классам и перемещаем их в соответсвующие папки.\n",
        "2. Делаем обработку проще с переводом изображений в черно-белое с CV2.\n",
        "3. Стандартизируем размер изображений (228, 228)."
      ],
      "metadata": {
        "id": "r7S78mUwo3FX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ka3DVDAg1nfv",
        "outputId": "1bfc5c97-a2da-4fd1-c0eb-2a90ee49c973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ignored file: 2?_4566.jpeg\n",
            "Ignored file: 0nan_5555.jpg\n",
            "Moved and processed 10097 files from /content/train/ to /content/prep_train1\n",
            "Ignored file: 0???.jpg\n",
            "Moved and processed 101 files from /content/test/ to /content/prep_test1\n",
            "completed\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "def preprocess_images(source_dir, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    class_folders = ['0', '1', '2', '3']\n",
        "    for class_folder in class_folders:\n",
        "        class_path = os.path.join(output_folder, class_folder)\n",
        "        if not os.path.exists(class_path):\n",
        "            os.makedirs(class_path)\n",
        "\n",
        "    moved_files = 0\n",
        "    for file_name in os.listdir(source_dir):\n",
        "        age_class = file_name.split('_')[0]\n",
        "        if age_class in class_folders:\n",
        "            source_path = os.path.join(source_dir, file_name)\n",
        "            destination_path = os.path.join(output_folder, age_class, file_name)\n",
        "\n",
        "            image = cv2.imread(source_path)\n",
        "            if image is not None:\n",
        "                resized_image = cv2.resize(image, (224, 224))\n",
        "                grayscale_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
        "                cv2.imwrite(destination_path, grayscale_image)\n",
        "                moved_files += 1\n",
        "            else:\n",
        "                print(f\"Failed to read image: {file_name}\")\n",
        "        else:\n",
        "            print(f\"Ignored file: {file_name}\")\n",
        "\n",
        "    print(f\"Moved and processed {moved_files} files from {source_dir} to {output_folder}\")\n",
        "\n",
        "\n",
        "train_dir = '/content/train/'\n",
        "test_dir = '/content/test/'\n",
        "\n",
        "prep_train1 = '/content/prep_train1'\n",
        "prep_test1 = '/content/prep_test1'\n",
        "\n",
        "preprocess_images(train_dir, prep_train1)\n",
        "preprocess_images(test_dir, prep_test1)\n",
        "\n",
        "print(\"completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3-шаг**: работа с моделью\n",
        "\n",
        "Подключаем **Tensorflow**.\n",
        "\n",
        "Используем более-менее известную модель **VGG19** (**EfficientB0** показал себя чуть хуже).\n",
        "\n",
        "0. Удаляем верхний слой модели для применения/адаптации своих настроек/слоев (ImageNet к примеру). Поэтому пишем `include_top=False`.\n",
        "\n",
        "1. Фиксируем веса модели, чтобы во время тренировки они не менялись.\n",
        "`base_model.trainable = False`\n",
        "\n",
        "2. Компилируем модель с оптимизатором `adam`, функцией потерь `categorical_crossentropy` и метрикой `accuracy`. Оптимизатор `adamW` не дал отличительных результатов.\n",
        "\n",
        "3. **ImageDataGen** отвечает за искусственное увеличение данных для компенсирования относительно маленьких датасетов.\n",
        "\n",
        "4. Генерируем дополнительные тестовые и тренировочные данные, для дальнейшей работы с метриками.\n",
        "\n",
        "**ModelCheckpoint** помогает нам сохранить лучшую модель по `val_accuracy`, **EarlyStopping** для предотвращения оверфиттинга, сравнимо с коробкой передач (в нашем случае стопаем после \"безрезультатных\" 10 эпох по `val_loss`)"
      ],
      "metadata": {
        "id": "EkWBt6QHo9c3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI8tK1zF1v82",
        "outputId": "e094f6cf-285c-4d9d-efbc-0e64fe38f875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80134624/80134624 [==============================] - 0s 0us/step\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
            "                                                                 \n",
            " global_average_pooling2d_1  (None, 512)               0         \n",
            "  (GlobalAveragePooling2D)                                       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 2052      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20026436 (76.39 MB)\n",
            "Trainable params: 2052 (8.02 KB)\n",
            "Non-trainable params: 20024384 (76.39 MB)\n",
            "_________________________________________________________________\n",
            "Found 10097 images belonging to 4 classes.\n",
            "Found 101 images belonging to 4 classes.\n",
            "Epoch 1/50\n",
            "316/316 [==============================] - 153s 461ms/step - loss: 1.3647 - accuracy: 0.3377 - val_loss: 1.2597 - val_accuracy: 0.4059 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "316/316 [==============================] - 142s 448ms/step - loss: 1.2473 - accuracy: 0.4206 - val_loss: 1.2013 - val_accuracy: 0.4752 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "316/316 [==============================] - 143s 451ms/step - loss: 1.2121 - accuracy: 0.4360 - val_loss: 1.1744 - val_accuracy: 0.4851 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "316/316 [==============================] - 138s 437ms/step - loss: 1.1951 - accuracy: 0.4529 - val_loss: 1.1575 - val_accuracy: 0.5149 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "316/316 [==============================] - 141s 445ms/step - loss: 1.1805 - accuracy: 0.4563 - val_loss: 1.1303 - val_accuracy: 0.5149 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "316/316 [==============================] - 143s 452ms/step - loss: 1.1811 - accuracy: 0.4526 - val_loss: 1.1428 - val_accuracy: 0.4752 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "316/316 [==============================] - 138s 438ms/step - loss: 1.1635 - accuracy: 0.4648 - val_loss: 1.1082 - val_accuracy: 0.5149 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "316/316 [==============================] - 143s 452ms/step - loss: 1.1636 - accuracy: 0.4684 - val_loss: 1.0916 - val_accuracy: 0.5347 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "316/316 [==============================] - 142s 449ms/step - loss: 1.1651 - accuracy: 0.4694 - val_loss: 1.1223 - val_accuracy: 0.5050 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "316/316 [==============================] - 139s 440ms/step - loss: 1.1534 - accuracy: 0.4741 - val_loss: 1.1121 - val_accuracy: 0.5149 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "316/316 [==============================] - 142s 450ms/step - loss: 1.1613 - accuracy: 0.4654 - val_loss: 1.1000 - val_accuracy: 0.5347 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "316/316 [==============================] - 140s 443ms/step - loss: 1.1538 - accuracy: 0.4739 - val_loss: 1.1092 - val_accuracy: 0.5149 - lr: 2.0000e-04\n",
            "Epoch 13/50\n",
            "316/316 [==============================] - 143s 451ms/step - loss: 1.1488 - accuracy: 0.4780 - val_loss: 1.1012 - val_accuracy: 0.5644 - lr: 2.0000e-04\n",
            "Epoch 14/50\n",
            "316/316 [==============================] - 141s 447ms/step - loss: 1.1428 - accuracy: 0.4744 - val_loss: 1.0962 - val_accuracy: 0.5545 - lr: 2.0000e-04\n",
            "Epoch 15/50\n",
            "316/316 [==============================] - 139s 441ms/step - loss: 1.1537 - accuracy: 0.4744 - val_loss: 1.0977 - val_accuracy: 0.5545 - lr: 4.0000e-05\n",
            "Epoch 16/50\n",
            "316/316 [==============================] - 140s 444ms/step - loss: 1.1491 - accuracy: 0.4725 - val_loss: 1.1013 - val_accuracy: 0.5446 - lr: 4.0000e-05\n",
            "Epoch 17/50\n",
            "316/316 [==============================] - 142s 448ms/step - loss: 1.1487 - accuracy: 0.4778 - val_loss: 1.0976 - val_accuracy: 0.5545 - lr: 4.0000e-05\n",
            "Epoch 18/50\n",
            "316/316 [==============================] - 138s 437ms/step - loss: 1.1479 - accuracy: 0.4717 - val_loss: 1.0976 - val_accuracy: 0.5545 - lr: 1.0000e-05\n",
            "Epoch 1/20\n",
            "316/316 [==============================] - 150s 459ms/step - loss: 1.0852 - accuracy: 0.5039 - val_loss: 0.9447 - val_accuracy: 0.5347 - lr: 1.0000e-04\n",
            "Epoch 2/20\n",
            "316/316 [==============================] - 147s 464ms/step - loss: 0.9094 - accuracy: 0.6011 - val_loss: 0.8872 - val_accuracy: 0.6634 - lr: 1.0000e-04\n",
            "Epoch 3/20\n",
            "316/316 [==============================] - 147s 464ms/step - loss: 0.8733 - accuracy: 0.6207 - val_loss: 0.7569 - val_accuracy: 0.7129 - lr: 1.0000e-04\n",
            "Epoch 4/20\n",
            "316/316 [==============================] - 146s 462ms/step - loss: 0.8178 - accuracy: 0.6366 - val_loss: 0.6712 - val_accuracy: 0.7129 - lr: 1.0000e-04\n",
            "Epoch 5/20\n",
            "316/316 [==============================] - 146s 463ms/step - loss: 0.7908 - accuracy: 0.6587 - val_loss: 0.7055 - val_accuracy: 0.6733 - lr: 1.0000e-04\n",
            "Epoch 6/20\n",
            "316/316 [==============================] - 148s 466ms/step - loss: 0.7672 - accuracy: 0.6598 - val_loss: 0.6109 - val_accuracy: 0.7426 - lr: 1.0000e-04\n",
            "Epoch 7/20\n",
            "316/316 [==============================] - 147s 466ms/step - loss: 0.7410 - accuracy: 0.6743 - val_loss: 0.6716 - val_accuracy: 0.7327 - lr: 1.0000e-04\n",
            "Epoch 8/20\n",
            "316/316 [==============================] - 147s 464ms/step - loss: 0.7238 - accuracy: 0.6840 - val_loss: 0.6191 - val_accuracy: 0.7327 - lr: 1.0000e-04\n",
            "Epoch 9/20\n",
            "316/316 [==============================] - 146s 463ms/step - loss: 0.7082 - accuracy: 0.6847 - val_loss: 0.6118 - val_accuracy: 0.7525 - lr: 1.0000e-04\n",
            "Epoch 10/20\n",
            "316/316 [==============================] - 143s 454ms/step - loss: 0.6412 - accuracy: 0.7156 - val_loss: 0.6489 - val_accuracy: 0.7525 - lr: 2.0000e-05\n",
            "Epoch 11/20\n",
            "316/316 [==============================] - 146s 459ms/step - loss: 0.6176 - accuracy: 0.7290 - val_loss: 0.6219 - val_accuracy: 0.7525 - lr: 2.0000e-05\n",
            "Epoch 12/20\n",
            "316/316 [==============================] - 145s 458ms/step - loss: 0.6106 - accuracy: 0.7253 - val_loss: 0.6242 - val_accuracy: 0.7426 - lr: 2.0000e-05\n",
            "Epoch 13/20\n",
            "316/316 [==============================] - 148s 468ms/step - loss: 0.5959 - accuracy: 0.7317 - val_loss: 0.6087 - val_accuracy: 0.7624 - lr: 1.0000e-05\n",
            "Epoch 14/20\n",
            "316/316 [==============================] - 146s 461ms/step - loss: 0.5910 - accuracy: 0.7365 - val_loss: 0.6264 - val_accuracy: 0.7624 - lr: 1.0000e-05\n",
            "Epoch 15/20\n",
            "316/316 [==============================] - 148s 468ms/step - loss: 0.5852 - accuracy: 0.7407 - val_loss: 0.5960 - val_accuracy: 0.7525 - lr: 1.0000e-05\n",
            "Epoch 16/20\n",
            "316/316 [==============================] - 147s 465ms/step - loss: 0.5734 - accuracy: 0.7421 - val_loss: 0.6383 - val_accuracy: 0.7624 - lr: 1.0000e-05\n",
            "Epoch 17/20\n",
            "316/316 [==============================] - 147s 463ms/step - loss: 0.5708 - accuracy: 0.7475 - val_loss: 0.5924 - val_accuracy: 0.7525 - lr: 1.0000e-05\n",
            "Epoch 18/20\n",
            "316/316 [==============================] - 149s 471ms/step - loss: 0.5732 - accuracy: 0.7472 - val_loss: 0.5949 - val_accuracy: 0.7525 - lr: 1.0000e-05\n",
            "Epoch 19/20\n",
            "316/316 [==============================] - 147s 465ms/step - loss: 0.5567 - accuracy: 0.7538 - val_loss: 0.6109 - val_accuracy: 0.7624 - lr: 1.0000e-05\n",
            "Epoch 20/20\n",
            "316/316 [==============================] - 146s 460ms/step - loss: 0.5617 - accuracy: 0.7499 - val_loss: 0.6182 - val_accuracy: 0.7822 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dropout(0.5),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/prep_train1',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    '/content/prep_test1',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint('/content/best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max')\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=test_generator,\n",
        "    callbacks=[checkpoint, early_stop, reduce_lr]\n",
        ")\n",
        "\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:15]:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_fine = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=test_generator,\n",
        "    callbacks=[checkpoint, early_stop, reduce_lr]\n",
        ")\n",
        "\n",
        "model.save('/content/model_fine_tuned.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4-шаг**: testing\n",
        "\n",
        "Тестируем модель на входных данных: загружаем изображение и стандартизируем. Выводим результат."
      ],
      "metadata": {
        "id": "A94YJUvxQT06"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rht29qjK2ppi",
        "outputId": "2789eadc-5e83-4c86-a8d9-c640ee751cca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 778ms/step\n",
            "Predicted сlass: [3]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "loaded_model = tf.keras.models.load_model('/content/model_fine_tuned.h5')\n",
        "\n",
        "img_path1 = '/content/nazik.PNG'\n",
        "img = image.load_img(img_path1, target_size=(224, 224))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "img_array /= 255.0\n",
        "\n",
        "prediction = loaded_model.predict(img_array)\n",
        "predicted_class = np.argmax(prediction, axis=1)\n",
        "print(f'Predicted сlass: {predicted_class}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5-шаг**: переводим модель в ONNX формат"
      ],
      "metadata": {
        "id": "TVnN4LuePtEy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6AEDSB4zH1t",
        "outputId": "0e686bec-6d76-468f-bdc0-1c3b76f27adc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/model_fine_tuned.onnx\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import onnx\n",
        "import tf2onnx\n",
        "\n",
        "model = load_model('/content/model_fine_tuned.h5')\n",
        "\n",
        "onnx_model_path = \"/content/model_fine_tuned.onnx\"\n",
        "spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name=\"input\"),)\n",
        "model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13)\n",
        "with open(onnx_model_path, \"wb\") as f:\n",
        "    f.write(model_proto.SerializeToString())\n",
        "\n",
        "print(f\"Model saved to {onnx_model_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6-шаг**: пишем примитивный интерфейс для нашей модели\n",
        "\n",
        "Anvil - платформа, которая дает возможность написать\n",
        "интерфейс на Python. Мы можем связать нашу модель с сервером и запустить. Код самого интерфейса прилагается.\n",
        "\n",
        "Приложение доступно по [ссылке](https://testovoepredict.anvil.app/), нужно лишь загрузить готовую модель в колаб и запустить код ниже:  "
      ],
      "metadata": {
        "id": "tNURxwJGSAzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anvil-uplink"
      ],
      "metadata": {
        "id": "1wmkGbWPWRjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "mYYUpKXv6WRw",
        "outputId": "0fc65517-c159-4464-9de4-a47ae5841184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"Default Environment\" as SERVER\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-eefd6aff5429>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0manvil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anvil/server.py\u001b[0m in \u001b[0;36mwait_forever\u001b[0;34m()\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import anvil.server\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "anvil.server.connect(\"server_Q4MJELTUXJQKASIXWVD54MOD-LHRD3LZOPXWXBF2S\")\n",
        "\n",
        "onnx_model_path = '/content/model_fine_tuned.onnx'\n",
        "ort_session = ort.InferenceSession(onnx_model_path)\n",
        "\n",
        "def preprocess_image(file):\n",
        "    file_bytes = file.get_bytes()\n",
        "    np_arr = np.frombuffer(file_bytes, np.uint8)\n",
        "    image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
        "    resized_image = cv2.resize(image, (224, 224))\n",
        "    grayscale_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
        "    normalized_image = grayscale_image / 255.0\n",
        "    reshaped_image = np.expand_dims(normalized_image, axis=-1)\n",
        "    input_image = np.repeat(reshaped_image, 3, axis=-1)\n",
        "    input_image = np.expand_dims(input_image, axis=0).astype(np.float32)\n",
        "    return input_image\n",
        "\n",
        "@anvil.server.callable\n",
        "def predict_item(file):\n",
        "    input_image = preprocess_image(file)\n",
        "    ort_inputs = {ort_session.get_inputs()[0].name: input_image}\n",
        "    ort_outs = ort_session.run(None, ort_inputs)\n",
        "    predicted_class = np.argmax(ort_outs[0])\n",
        "    return str(predicted_class)\n",
        "\n",
        "anvil.server.wait_forever()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "model = tf.keras.models.load_model('/content/model_fine_tuned.h5')\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = model.predict(test_generator)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(y_true, y_pred_classes, target_names=test_generator.class_indices.keys())\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "cWXST9PG468m",
        "outputId": "5ada29db-ff32-41bc-cb53-f37547184903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Layer \"dense_1\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<KerasTensor shape=(None, 7, 7, 512), dtype=float32, sparse=False, name=keras_tensor_33>, <KerasTensor shape=(None, 7, 7, 512), dtype=float32, sparse=False, name=keras_tensor_34>]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4d1093a8373e>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/model_fine_tuned.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Predict the labels for the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return legacy_h5_format.load_model_from_hdf5(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/legacy_h5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msaving_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_option_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_legacy_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             model = saving_utils.model_from_config(\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/saving_utils.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_replace_nested_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     return serialization.deserialize_keras_object(\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODULE_OBJECTS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/legacy/saving/serialization.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"custom_objects\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                 deserialized_obj = cls.from_config(\n\u001b[0m\u001b[1;32m    496\u001b[0m                     \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                     custom_objects={\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 )\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         if (\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer, rebuild)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrebuild\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_rebuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36m_maybe_rebuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_lock_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36mbuild_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0moriginal_build_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m             \u001b[0;31m# Record build config.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_build_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;31m# Can happen if shape inference is not implemented.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;34mf'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;34mf\" but it received {len(inputs)} input tensors. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Layer \"dense_1\" expects 1 input(s), but it received 2 input tensors. Inputs received: [<KerasTensor shape=(None, 7, 7, 512), dtype=float32, sparse=False, name=keras_tensor_33>, <KerasTensor shape=(None, 7, 7, 512), dtype=float32, sparse=False, name=keras_tensor_34>]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1P5SfYkx5KL4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}